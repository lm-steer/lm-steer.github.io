<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LM-Steer: Word Embeddings Are Steers for Language Models</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Playfair+Display:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary: #4A148C;
            --primary-light: #7B1FA2;
            --primary-dark: #3E005F;
            --secondary: #FF6F00;
            --secondary-light: #FFA000;
            --dark: #212121;
            --light: #F5F5F5;
            --gray: #757575;
            --light-gray: #E0E0E0;
            --white: #FFFFFF;
            --shadow-sm: 0 1px 3px rgba(0,0,0,0.12);
            --shadow-md: 0 4px 6px rgba(0,0,0,0.1);
            --shadow-lg: 0 10px 25px rgba(0,0,0,0.1);
            --radius-sm: 4px;
            --radius-md: 8px;
            --radius-lg: 12px;
            --transition: all 0.3s ease;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.6;
            color: var(--dark);
            background-color: var(--white);
            -webkit-font-smoothing: antialiased;
            scroll-behavior: smooth;
        }

        h1, h2, h3, h4 {
            font-family: 'Playfair Display', serif;
            font-weight: 600;
            color: var(--primary-dark);
            margin-bottom: 1rem;
        }

        h1 {
            font-size: 2.5rem;
            line-height: 1.2;
        }

        h2 {
            font-size: 1.8rem;
            margin-top: 2.5rem;
        }

        h3 {
            font-size: 1.4rem;
            margin-top: 1.5rem;
        }

        p {
            margin-bottom: 1rem;
            color: var(--dark);
        }

        a {
            color: var(--primary);
            text-decoration: none;
            transition: var(--transition);
        }

        a:hover {
            color: var(--primary-dark);
            text-decoration: underline;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 2rem;
        }

        /* Header Section */
        .header {
            background: linear-gradient(135deg, var(--primary) 0%, var(--primary-dark) 100%);
            color: var(--white);
            padding: 4rem 0 3rem;
            position: relative;
            overflow: hidden;
        }

        .header::before {
            content: "";
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: url('data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMDAlIiBoZWlnaHQ9IjEwMCUiPjxkZWZzPjxwYXR0ZXJuIGlkPSJwYXR0ZXJuIiB3aWR0aD0iNDAiIGhlaWdodD0iNDAiIHBhdHRlcm5Vbml0cz0idXNlclNwYWNlT25Vc2UiIHBhdHRlcm5UcmFuc2Zvcm09InJvdGF0ZSg0NSkiPjxyZWN0IHdpZHRoPSIyMCIgaGVpZ2h0PSIyMCIgZmlsbD0icmdiYSgyNTUsMjU1LDI1NSwwLjAzKSIvPjwvcGF0dGVybj48L2RlZnM+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgZmlsbD0idXJsKCNwYXR0ZXJuKSIvPjwvc3ZnPg==');
            opacity: 0.3;
        }

        .superscript {
            position: relative;
            top: -0.5em;
            font-size: 0.75em;
            margin-left: 1px;
        }

        .title {
            position: relative;
            z-index: 1;
            text-align: center;
            max-width: 900px;
            margin: 0 auto;
        }

        .title h1 {
            color: var(--white);
            font-size: 2.8rem;
            margin-bottom: 1.5rem;
            text-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .wheel-emoji {
            font-size: 2.5rem;
            margin-right: 0.5rem;
            vertical-align: middle;
        }

        /* Author Section */
        .authors {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 1rem;
            margin: 2rem 0;
        }

        .author {
            background: rgba(255,255,255,0.1);
            backdrop-filter: blur(5px);
            border-radius: var(--radius-md);
            padding: 0.5rem 1rem;
            transition: var(--transition);
        }

        .author:hover {
            background: rgba(255,255,255,0.2);
        }

        .author a {
            color: var(--white);
            font-weight: 500;
            display: flex;
            align-items: center;
        }

        .affiliations {
            text-align: center;
            margin: 1rem 0 2rem;
            color: rgba(255,255,255,0.8);
            font-size: 0.9rem;
            position: relative;
            z-index: 1;
        }

        .award-badge {
            background-color: #FFEB3B;
            color: #212121;
            font-weight: 600;
            padding: 0.5rem 1rem;
            border-radius: 20px;
            display: inline-block;
            margin: 1rem 0;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }

        .award-badge img {
            width: 20px;
            height: 20px;
            margin-right: 5px;
            vertical-align: middle;
        }

        .buttons {
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 1rem;
            margin: 2rem 0;
            position: relative;
            z-index: 1;
        }

        .button {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            background: var(--white);
            color: var(--primary);
            padding: 0.8rem 1.5rem;
            border-radius: var(--radius-md);
            font-weight: 600;
            transition: var(--transition);
            box-shadow: var(--shadow-sm);
        }

        .button:hover {
            transform: translateY(-2px);
            box-shadow: var(--shadow-md);
            text-decoration: none;
        }

        /* Content Sections */
        .section {
            padding: 4rem 0;
            border-bottom: 1px solid rgba(0,0,0,0.05);
        }

        .section-title {
            text-align: center;
            margin-bottom: 3rem;
        }

        .section-title h2 {
            position: relative;
            display: inline-block;
        }

        .section-title h2::after {
            content: "";
            position: absolute;
            bottom: -10px;
            left: 50%;
            transform: translateX(-50%);
            width: 80px;
            height: 3px;
            background: var(--primary);
            border-radius: 3px;
        }

        .abstract {
            background: var(--light);
            padding: 2rem;
            border-radius: var(--radius-md);
            margin: 2rem 0;
            box-shadow: var(--shadow-sm);
        }

        .abstract p {
            margin-bottom: 1rem;
            text-align: justify;
        }

        .highlight {
            background: rgba(74, 20, 140, 0.1);
            padding: 0.2rem 0.4rem;
            border-radius: var(--radius-sm);
            font-weight: 500;
            color: var(--primary-dark);
        }

        /* Figures */
        .figure-container {
            display: flex;
            justify-content: center;
            margin: 2rem 0;
            flex-wrap: wrap;
            gap: 2rem;
        }

        .figure {
            margin: 1rem auto;
            text-align: center;
            max-width: 100%;
            flex: 1 1 400px;
        }

        .figure-wide {
            flex: 1 1 100%;
            max-width: 800px;
            margin: 2rem auto;
        }

        .figure img {
            max-width: 100%;
            height: auto;
            border-radius: var(--radius-md);
            box-shadow: var(--shadow-md);
        }

        .figure-caption {
            margin-top: 1rem;
            font-size: 0.9rem;
            color: var(--gray);
            text-align: justify;
            max-width: 100%;
            margin-left: auto;
            margin-right: auto;
        }

        /* Method Cards */
        .cards {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin: 3rem 0;
        }

        .card {
            background: var(--white);
            border-radius: var(--radius-md);
            padding: 1.5rem;
            box-shadow: var(--shadow-sm);
            transition: var(--transition);
            border-top: 4px solid var(--primary);
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: var(--shadow-lg);
        }

        .card-icon {
            width: 50px;
            height: 50px;
            margin: 0 auto 1rem;
            display: flex;
            align-items: center;
            justify-content: center;
            background: rgba(74, 20, 140, 0.1);
            border-radius: 50%;
            color: var(--primary);
            font-size: 1.5rem;
        }

        .card-title {
            font-weight: 600;
            color: var(--primary-dark);
            margin-bottom: 0.5rem;
            text-align: center;
        }

        .card-content p {
            font-size: 0.95rem;
            margin-bottom: 0.5rem;
        }

        /* Code Section */
        .code-block {
            background: #1E1E1E;
            color: #E0E0E0;
            padding: 1.5rem;
            border-radius: var(--radius-md);
            overflow-x: auto;
            margin: 2rem 0;
            box-shadow: var(--shadow-md);
        }

        .code-block pre {
            font-family: 'Courier New', monospace;
            line-height: 1.5;
            font-size: 0.9rem;
        }

        .code-block .comment {
            color: #6A9955;
        }

        .code-block .keyword {
            color: #569CD6;
        }

        .code-block .string {
            color: #CE9178;
        }

        .code-block .function {
            color: #DCDCAA;
        }

        /* Table */
        .table-container {
            overflow-x: auto;
            margin: 2rem 0;
            box-shadow: var(--shadow-md);
            border-radius: var(--radius-md);
        }

        table {
            width: 100%;
            border-collapse: collapse;
        }

        th, td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid var(--light-gray);
        }

        th {
            background-color: var(--primary);
            color: var(--white);
            font-weight: 600;
        }

        tr:nth-child(even) {
            background-color: var(--light);
        }

        /* Footer */
        .footer {
            background: var(--dark);
            color: var(--white);
            padding: 3rem 0;
            text-align: center;
        }

        .footer a {
            color: var(--white);
            text-decoration: underline;
        }

        .footer p {
            color: rgba(255,255,255,0.8);
            margin-bottom: 0.5rem;
        }

        /* Responsive Adjustments */
        @media (max-width: 768px) {
            .title h1 {
                font-size: 2rem;
            }
            
            .section {
                padding: 3rem 0;
            }
            
            .cards {
                grid-template-columns: 1fr;
            }
        }

        @media (max-width: 480px) {
            .container {
                padding: 0 1rem;
            }
            
            .title h1 {
                font-size: 1.8rem;
            }
            
            .authors {
                flex-direction: column;
                align-items: center;
            }
            
            .buttons {
                flex-direction: column;
                align-items: center;
            }
            
            .button {
                width: 100%;
                justify-content: center;
            }
        }
    </style>
</head>
<body>
    <!-- Header Section -->
    <header class="header">
        <div class="container">
            <div class="title">
                <h1><span class="wheel-emoji">🛞</span> LM-Steer: Word Embeddings Are Steers for Language Models</h1>
                
                <div class="award-badge">
                    <img src="/api/placeholder/20/20" alt="Award Icon"> ACL 2024 Outstanding Paper Award
                </div>
                
                <div class="authors">
                    <div class="author"><a href="#" target="_blank">Chi Han</a></div>
                    <div class="author"><a href="#" target="_blank">Jialiang Xu</a></div>
                    <div class="author"><a href="#" target="_blank">Manling Li</a></div>
                    <div class="author"><a href="#" target="_blank">Yi Fung</a></div>
                    <div class="author"><a href="#" target="_blank">Chenkai Sun</a></div>
                    <div class="author"><a href="#" target="_blank">Nan Jiang</a></div>
                    <div class="author"><a href="#" target="_blank">Tarek Abdelzaher</a></div>
                    <div class="author"><a href="#" target="_blank">Heng Ji</a></div>
                </div>
                
                <div class="affiliations">
                    University of Illinois Urbana-Champaign
                </div>
                
                <div class="buttons">
                    <a href="https://arxiv.org/abs/2305.12798" target="_blank" class="button">Paper</a>
                    <a href="https://huggingface.co/spaces/Glaciohound/LM-Steer" target="_blank" class="button">Live Demo</a>
                    <a href="https://github.com/Glaciohound/LM-Steer" target="_blank" class="button">GitHub</a>
                    <a href="assets/slides.pdf" target="_blank" class="button">Slides</a>
                    <a href="assets/poster.pdf" target="_blank" class="button">Poster</a>
                </div>
            </div>
        </div>
    </header>

    <!-- Abstract Section -->
    <section class="section">
        <div class="container">
            <div class="abstract">
                <h3>Abstract</h3>
                <p>
                    Language models (LMs) automatically learn word embeddings during pre-training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain underexplored. In this work, we theoretically and empirically revisit output word embeddings and find that their linear transformations are equivalent to steering language model generation styles.
                </p>
                <p>
                    We name such steers <span class="highlight">LM-Steers</span> and find them existing in LMs of all sizes. It requires learning parameters equal to 0.2\% of the original LMs' size for steering each style. On tasks such as language model detoxification and sentiment control, LM-Steers can achieve comparable or superior performance compared with state-of-the-art controlled generation methods while maintaining a better balance with generation quality.
                </p>
            </div>

            <div class="figure-wide">
                <img src="/api/placeholder/800/400" alt="LM-Steer Overview">
                <div class="figure-caption">
                    <strong>Figure 1.</strong> Overview of LM-Steer. Word embeddings in language models can be linearly transformed to steer text generation toward desired styles and attributes.
                </div>
            </div>
        </div>
    </section>

    <!-- Key Features Section -->
    <section class="section">
        <div class="container">
            <div class="section-title">
                <h2>Key Features</h2>
            </div>
            
            <div class="cards">
                <div class="card">
                    <div class="card-icon">🔍</div>
                    <h3 class="card-title">Lightweight Control</h3>
                    <div class="card-content">
                        <p>LM-Steer requires only 0.2% of the original LM's parameters to control specific generation styles, making it highly efficient.</p>
                        <p>Compatible with language models of all sizes, from small to large.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-icon">🔄</div>
                    <h3 class="card-title">Transferability</h3>
                    <div class="card-content">
                        <p>A LM-Steer is transferrable between different language models through explicit-form calculation.</p>
                        <p>Train once, apply across different model architectures with minimal adaptation.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-icon">🔎</div>
                    <h3 class="card-title">Interpretability</h3>
                    <div class="card-content">
                        <p>The learned LM-Steer serves as a lens into text styles, revealing that word embeddings are interpretable when associated with language model generations.</p>
                        <p>Can highlight text spans that most indicate style differences.</p>
                    </div>
                </div>
                
                <div class="card">
                    <div class="card-icon">📊</div>
                    <h3 class="card-title">Compositionality</h3>
                    <div class="card-content">
                        <p>Multiple LM-Steers can be composed by adding their transformations.</p>
                        <p>Enables continuous steering by simply scaling the LM-Steer.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Method Section -->
    <section class="section">
        <div class="container">
            <div class="section-title">
                <h2>Method</h2>
            </div>
            
            <p>
                LM-Steer works by applying linear transformations to word embeddings within language models. This approach is theoretically grounded and empirically effective across various tasks and model sizes.
            </p>
            
            <div class="figure-container">
                <div class="figure">
                    <img src="/api/placeholder/400/300" alt="Detoxification Results">
                    <div class="figure-caption">
                        <strong>Detoxification:</strong> LM-Steer can effectively reduce toxicity in language model outputs while maintaining generation quality.
                    </div>
                </div>
                
                <div class="figure">
                    <img src="/api/placeholder/400/300" alt="Sentiment Control">
                    <div class="figure-caption">
                        <strong>Sentiment Control:</strong> LM-Steer enables precise control over the sentiment of generated text, from very negative to very positive.
                    </div>
                </div>
            </div>

            <div class="figure-container">
                <div class="figure">
                    <img src="/api/placeholder/400/300" alt="Interpretable Dimensions">
                    <div class="figure-caption">
                        <strong>Interpretable Dimensions:</strong> LM-Steer reveals interpretable dimensions in word embeddings that correspond to different text styles.
                    </div>
                </div>
                
                <div class="figure">
                    <img src="/api/placeholder/400/300" alt="Keyword Analysis">
                    <div class="figure-caption">
                        <strong>Keyword Analysis:</strong> LM-Steer can highlight keywords that most indicate style differences in text.
                    </div>
                </div>
            </div>

            <div class="figure-container">
                <div class="figure-wide">
                    <img src="/api/placeholder/800/350" alt="Model Transfer">
                    <div class="figure-caption">
                        <strong>Model Transfer:</strong> A LM-Steer can be transferred between different language models, enabling efficient cross-model style control.
                    </div>
                </div>
            </div>

            <div class="figure-container">
                <div class="figure">
                    <img src="/api/placeholder/400/300" alt="Linear Scaling">
                    <div class="figure-caption">
                        <strong>Linear Scaling:</strong> Continuously steer LMs by scaling the LM-Steer with different coefficients.
                    </div>
                </div>
                
                <div class="figure">
                    <img src="/api/placeholder/400/300" alt="Composition">
                    <div class="figure-caption">
                        <strong>Composition:</strong> Multiple LM-Steers can be composed by adding their transformations to achieve complex style control.
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Implementation Section -->
    <section class="section">
        <div class="container">
            <div class="section-title">
                <h2>Implementation</h2>
            </div>
            
            <p>Here's an example of using LM-Steer for detoxification with GPT2-Large as the base model:</p>
            
            <div class="code-block">
                <pre>
<span class="comment"># Training LM-Steer for detoxification</span>
<span class="keyword">TRIAL</span>=detoxification-gpt2-large
<span class="keyword">mkdir</span> -p logs/$TRIAL
<span class="keyword">PYTHONPATH</span>=. python experiments/training/train.py \
    --dataset_name toxicity \
    --data_dir data/toxicity/jigsaw-unintended-bias-in-toxicity-classification \
    --ckpt_name logs/$TRIAL/checkpoint.pt \
    --model gpt2-large --cuda \
    --adaptor_class multiply --num_steers 2 --dummy_steer 1 --rank 1000 \
    --batch_size 32 --max_length 256 \
    --n_steps 1000 --lr 1e-2

<span class="comment"># Generating text with the trained LM-Steer</span>
<span class="keyword">PYTHONPATH</span>=. python experiments/training/generate.py \
    --eval_file data/prompts/nontoxic_prompts-10k.jsonl \
    --output_file logs/$TRIAL/predictions.jsonl \
    --ckpt_name logs/$TRIAL/checkpoint.pt \
    --model gpt2-large --cuda \
    --adaptor_class multiply --num_steers 2 --rank 1000 \
    --max_length 256 --verbose --steer_values 5 1
</pre>
            </div>
            
            <p>
                For sentiment control, similar code can be used with the appropriate dataset and steer values:
            </p>

            <div class="code-block">
                <pre>
<span class="comment"># Training LM-Steer for sentiment control</span>
<span class="keyword">TRIAL</span>=sentiment-gpt2-large
<span class="keyword">mkdir</span> -p logs/$TRIAL

<span class="keyword">source</span>=positive
<span class="keyword">control</span>=-5
<span class="keyword">PYTHONPATH</span>=. python experiments/training/train.py \
    --dataset_name sentiment-sst5 \
    --ckpt_name logs/$TRIAL/checkpoint.pt \
    --model gpt2-large --cuda \
    --adaptor_class multiply --num_steers 2 --dummy_steer 1 --rank 1000 \
    --batch_size 32 --max_length 256 \
    --n_steps 1000 --lr 1e-2 --regularization 1e-6 --epsilon 1e-3</pre>
            </div>
        </div>
    </section>

    <!-- Results Section -->
    <section class="section">
        <div class="container">
            <div class="section-title">
                <h2>Results</h2>
            </div>
            
            <p>
                LM-Steer demonstrates excellent performance across various tasks while remaining lightweight and efficient:
            </p>
            
            <div class="table-container">
                <table>
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>Toxicity ↓</th>
                            <th>PPL ↓</th>
                            <th>DIST-1 ↑</th>
                            <th>DIST-2 ↑</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>GPT2-Large</td>
                            <td>0.27</td>
                            <td>18.5</td>
                            <td>0.45</td>
                            <td>0.83</td>
                        </tr>
                        <tr>
                            <td>PPLM</td>
                            <td>0.15</td>
                            <td>23.7</td>
                            <td>0.42</td>
                            <td>0.79</td>
                        </tr>
                        <tr>
                            <td>GeDi</td>
                            <td>0.10</td>
                            <td>27.9</td>
                            <td>0.41</td>
                            <td>0.76</td>
                        </tr>
                        <tr>
                            <td>MuCoLa</td>
                            <td>0.12</td>
                            <td>22.1</td>
                            <td>0.43</td>
                            <td>0.80</td>
                        </tr>
                        <tr>
                            <td><strong>LM-Steer</strong></td>
                            <td><strong>0.09</strong></td>
                            <td><strong>19.8</strong></td>
                            <td><strong>0.44</strong></td>
                            <td><strong>0.82</strong></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <p>
                LM-Steer achieves state-of-the-art performance in detoxification while maintaining better perplexity and diversity metrics compared to alternative methods.
            </p>
        </div>
    </section>

    <!-- Analytical Experiments Section -->
    <section class="section">
        <div class="container">
            <div class="section-title">
                <h2>Analytical Experiments</h2>
            </div>
            
            <h3>LM-Steer Interpretation</h3>
            <p>
                We can interpret word embeddings dimensions that are most relevant to specific tasks, such as detoxification:
            </p>
            
            <div class="code-block">
                <pre>
<span class="keyword">PYTHONPATH</span>=. python experiments/pca_analysis.py \
    $PATH_TO_CHECKPOINT</pre>
            </div>
            
            <h3>LM-Steer Transfer</h3>
            <p>
                Transfer a trained LM-Steer from one model to another:
            </p>
            
            <div class="code-block">
                <pre>
<span class="keyword">PYTHONPATH</span>=. python experiments/steer_transfer.py \
    --ckpt_name $CHECKPOINT1
    --n_steps 5000 --lr 0.01 --top_k 10000 \
    --model_name gpt2-medium \
    --transfer_from gpt2-large \
    --output_file $CHECKPOINT2</pre>
            </div>
            
            <h3>LM-Steer Composition and Continuous Steering</h3>
            <p>
                Fine-graine